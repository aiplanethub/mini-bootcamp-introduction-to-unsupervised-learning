# Evaluating Unsupervised Learning Models

## Learning Objectives

* Model Evaluation
* External Validation Techniques
* Internal Validation Techniques

## Model Evaluation

When you talk about validating or evaluating a machine learning model, it’s essential to know that the validation techniques employed not only help in measuring performance but also go a long way in helping you understand your model on a deeper level. This is why much time is devoted to the process of result validation and model evaluation while building a machine-learning model.

Result validation is crucial as it ensures that our model gives good results not just on the training data but, more importantly, on the live or test data.

## How is evaluation here different from supervised learning?

In the case of supervised learning, evaluation is done mainly by measuring the performance metrics, such as accuracy, precision, recall, AUC, etc., on the training and the holdout sets. Such performance metrics help in deciding model viability. Then we may tune the hyper parameters and repeat the same process till we achieve the desired performance.

However, in the case of unsupervised learning, the process is not very straightforward as we do not have the ground truth (the labels). In the absence of labels, it is very diﬃcult to validate our results.

Thus, it is difficult to evaluate the quality of an unsupervised algorithm due to the absence of an explicit goodness metric, unlike supervised learning.

## Existing Domain Knowledge

Let’s say we have a problem at hand to **cluster different songs in Spotify together based on genres to create playlists**. After our work is done, how do we know it is good enough?

We can verify the results of our clustering exercise through our existing knowledge of the data (for example, knowing that genre A and genre B of music are similar, so if those clusters are close together, it should be correct).

But what if we don’t have such prior knowledge of our data? What if the data isn’t even labeled (as is the case in many real-life clustering cases)? Even if it is, what if these labels are initially meaningless to us? There are plenty of artists we’ve never even heard of, and if we’re trying to group thousands of tracks, then it’s impractical to verify every cluster manually. In these cases, we need some mathematical measure for how ‘successful’ our clustering has been.

### Example

Returning to our objective of creating clusters based on genres for Spotify playlists, we have learned several clustering algorithms till now, so we can try each of them out to create clusters.

So let’s say we implemented four algorithms -

Algo 1: Hierarchical Agglomerative Clustering with ward linkage

Algo 2: Hierarchical Agglomerative Clustering with complete linkage

Algo 3: Hierarchical Agglomerative Clustering with average linkage

Algo 4: K-Means Clustering

Now, we need to know which one is performing the best on our data.

## Evaluation Techniques

There are two classes of statistical techniques to validate results for cluster learning. These are:

1. External validation
2. Internal validation

## **External Validation**

**Metrics where original labels are required to evaluate clusters.**  
We can carry out this type of validation if true cluster labels are available.  
In this approach, we will have a set of clusters S = {C1, C2, C3,..., Cn }, generated from some clustering algorithm. We will have another set of clusters, P = {D1, D2, D3,..., Dm}, representing the true cluster labels on the same data. The idea is to measure the statistical similarity between the two sets. A cluster set is considered good if it is highly similar to the true one.  
To measure the similarity between S and P, we label each pair of records from data as Positive if the pairs belong to the same cluster in P, else Negative. A similar exercise is carried out for S as well. We then compute a confusion matrix between pair labels of S and P, which we can use to measure the similarity.

![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_5a93044bd07d4bc8b31359d0abf18cbc.png)

* TP: Number of pairs of records in the same cluster for both S and P.
* FP: Number of pairs of records in the same cluster in S but not in P.
* FN: Number of pairs of records in the same cluster in P but not S.
* TN: Number of pairs of records in different clusters in both S and P.

We can calculate different metrics on the above four indicators to estimate the similarity between S (cluster labels generated by unsupervised method) and P (true cluster labels). Some example metrics are Precision, Recall, and F1-score.

### Matrix Representation

We can represent our results in a matrix, showing what percentage of each playlist’s songs have ended up in each cluster.  
If the clustering had been perfect, we’d expect each row and each column of the matrix to contain exactly one entry of 100% (it needn’t be in a diagonal, of course, since the cluster name assignment is arbitrary).

#### Matrix Representation for Algo 1


![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_d8397bbf1b0f4e8a937902ac731aba4f.png)


The default ‘ward’ linkage, which tries to minimize variance within clusters, has done an excellent job with all four genres. However, there is some leakage into cluster B, i.e., in the 2nd column, there are entries in multiple clusters, not just one.

#### Matrix Representation for Algo 2




![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_8455459d6f2a4cd6b1e26251644e6304.png)




‘Complete’ linkage has not worked well. It has placed much of the dataset into cluster A. Cluster C consists of a single rap song.

#### Matrix Representation for Algo 3




![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_8b6c851c200c42f795c7d75b1cbc9d64.png)





‘Average’ linkage has similar issues to the ‘Complete’ linkage. It placed many data points into a single cluster, with two clusters consisting of a single song.

#### Matrix Representation for Algo 4




![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_1cc54508511c425ea9b3c0dca075443e.png)





As with the HAC algorithm using ‘ward’ linkage, K-Means clustering has done an excellent job across most of the algorithms, with some jazz and rap songs being ‘mistaken’ for K-Pop.

While these matrices are suitable for ‘eyeballing’ our results, they’re far from mathematically rigorous. Let’s consider some metrics to help assign a number to our cluster quality.

### Adjusted Rand Index

The Adjusted Rand Index attempts to express the proportion of ‘correct’ cluster assignments. It computes a similarity measure between two different clustering methods by considering all pairs of samples and counting pairs assigned in the same or different clusters predicted, against the true cluster labels, adjusting for random chance.

This metric (as well as the other metrics we’ll consider) can be evaluated using Scikit-Learn.

![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_a3a7a0b860e340449a9c496337b2a808.png)

The Adjusted Rand index is bounded between -1 and 1. Closer to 1 is good, while closer to -1 is bad.



![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_a26709e8ba144f6fb07d41b84551e31a.png)



We see that K-Means and Ward Linkage have high scores. We’d expect this based on the matrices we previously observed.

### Fowlkes Mallows Score

The Fowlkes Mallow Score is similar to the Adjusted Rand Index in that it tells you the degree to which cluster assignments are ‘correct’.

In particular, it calculates the geometric mean (a special type of average where we multiply the numbers together and then take a square root (for two numbers)) between precision and recall. It’s bounded between 0 and 1, with higher values being better.


![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_dddc7808500d41f28f00c4f3bfbfdefe.png)


![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_7edf7f85006f4fe5ad32c5720a29ac2e.png)


We have similar rankings to the Adjusted Rand Index — which we would expect, given that they’re two methods of trying to answer the same question.

### More external validation techniques

A few more external validation techniques include:

* Jaccard Similarity
* Mutual Information

### Drawbacks of External Validation

Business/User validation, as the name suggests, requires external inputs to the data.

The idea is to generate clusters based on the knowledge of subject matter experts and then evaluate the similarity between the two sets of clusters, i.e., the clusters generated by ML and clusters generated as a result of human inputs.

However, in most cases, such knowledge is not readily available. Also, this approach is not very scalable. Hence, in practice, external validation is usually skipped.

## **Internal validation**

**Metrics where original labels are not required to evaluate clusters.**

### Why Internal Validation?

Given that dealing with unlabelled data is one of the primary use cases of unsupervised learning, we require some other metrics that evaluate clustering results without needing to refer to ‘true’ labels.

### How Internal Validation?

Most of the literature related to internal validation for cluster learning revolves around the following two types of metrics –

1. Cohesion within each cluster
2. Separation between different clusters

![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_dc913cc422014386a53562806dc172b7.png)

### Intuition

Suppose we have the following results from 3 separate clustering analyses.










![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_a25889fa4ee64bd4ae2a770ee310cdac.png)









![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_88f1bfeca0ec45cc887cb0c01bb125fa.png)








![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_3849a2bbeb694f28ab655922514dd9ba.png)






The ‘tighter’ we can make our clusters, the better. Is there some way to give a number to this idea of ‘tightness’?

### Internal Validation Metrics

Instead of using two metrics, several measures are available that combine cohesion and coupling into a single measure. A few examples of such measures are:

* Silhouette coefficient
* Calisnki-Harabasz coefficient
* Dunn index
* Xie-Beni score
* Hartigan index

#### Silhouette Score

The Silhouette Score describes how similar a data point is to other data points in its cluster relative to data points not in its cluster (this is aggregated over all data points to get the score for an overall clustering). In other words, it thinks about how ‘distinct’ the clusters are in space — indeed, one could use any measure of ‘distance’ to calculate the score.

It is bounded between -1 and 1. Closer to -1 suggests incorrect clustering, while closer to +1 shows that each cluster is very dense.

![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_d1709b643653495e9096653106f0f16c.png)


![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_a72eca486f4b44c69cd346598f0a233d.png)



We see that none of the clusters have super-high Silhouette Scores. Interestingly, we see that the Average Linkage clusters have the highest scores. Remember, however, that this algorithm produced two clusters containing just a single data point. This is unlikely to be a desirable outcome in a real-world situation (a lesson that you often can’t rely on a single metric to make decisions about the quality of an algorithm!).

#### Calinski Harabaz Index

The Calinski Harabaz Index is the ratio of the variance of a data point compared to points in other clusters against the variance compared to points within its cluster.

Since we want this first part to be high and the second to be low, a **high CH index is desirable**. Unlike other metrics we have seen, this score is not bounded.

![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_61a9a0324af64134824dffe94ac0acc8.png)

![image.png](https://dphi-live.s3.amazonaws.com/media_uploads/image_e98f806808f84eebab77911acd038197.png)

Here we see that our K-Means and Ward Linkage algorithms score highly. The Complete and Average linkage algorithms are punished for having one or two large clusters, which will have a higher level of intra-variance.

#### Additional Exploration

You might want to explore a technique called ‘Twin-Sample Validation’.

It should be used in combination with internal validation and can prove to be highly useful in the case of time-series data where we want to ensure that our results remain the same across time. (If you want to learn more about time-series analysis, check out our [Course on Introduction to Time Series Analysis](https://dphi.tech/courses/introduction-to-time-series-analysis).)

## Conclusion

So this brings us to the end of this unit.

The purpose of this unit was to cover the most important and most commonly used model evaluation metrics in machine learning and clarify the meaning of these metrics. I hope this might have helped you and motivated you to pick the right metric for your use case, to evaluate the goodness of the machine learning model you built.